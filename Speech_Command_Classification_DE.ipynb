{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzkLHet6JIPX",
        "outputId": "17f4607f-6a1a-41dc-fe00-dfb3e9306004"
      },
      "outputs": [],
      "source": [
        "!pip install python_speech_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khcp0u_yJMQL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,LSTM,BatchNormalization,GlobalAveragePooling2D\n",
        "from keras.layers import Dropout,Dense,TimeDistributed\n",
        "from keras.models import Sequential\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "from python_speech_features import mfcc\n",
        "import pickle\n",
        "from keras.callbacks import ModelCheckpoint\n",
        " \n",
        "import librosa as lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3wH84isvy6rt",
        "outputId": "dd2c97a2-3df5-471a-e573-7714e5ca14f2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPKbqPPdJSEW",
        "outputId": "10ecc694-864f-4ac2-ae3b-2e53120c5150"
      },
      "outputs": [],
      "source": [
        "data_dir = pathlib.Path('data/mini_speech_commands')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')\n",
        " \n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)\n",
        " \n",
        " \n",
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)\n",
        "print('Number of examples per label:',\n",
        "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
        "print('Example file tensor:', filenames[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfIB-0XhJWRn",
        "outputId": "55bbf5e4-299f-4e89-bd87-a3ac626be622"
      },
      "outputs": [],
      "source": [
        "train_files = filenames[:6400]\n",
        "val_files = filenames[6400: 6400 + 1000]\n",
        "test_files = filenames[-600:]\n",
        " \n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))\n",
        " \n",
        " \n",
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)\n",
        " \n",
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        " \n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kut1NFEwJZk5",
        "outputId": "c3b14a16-52a3-4693-b49c-03289232408b"
      },
      "outputs": [],
      "source": [
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  print(\"label\")\n",
        "  print(label)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  print(\"waveform\")\n",
        "  print(waveform)\n",
        "  return waveform, label\n",
        " \n",
        " \n",
        " \n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        " \n",
        " \n",
        " \n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
        "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  ax.plot(audio.numpy())\n",
        "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  ax.set_title(label)\n",
        " \n",
        "plt.show()\n",
        " \n",
        " \n",
        " \n",
        "def get_spectrogram(waveform):\n",
        "  # Padding for files with less than 16000 samples\n",
        "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        " \n",
        "  # Concatenate audio with padding so that all audio clips will be of the \n",
        "  # same length\n",
        "  waveform = tf.cast(waveform, tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "      \n",
        "  spectrogram = tf.abs(spectrogram)\n",
        " \n",
        "  return spectrogram\n",
        " \n",
        " \n",
        "for waveform, label in waveform_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        " \n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Audio playback')\n",
        "display.display(display.Audio(waveform, rate=16000))\n",
        " \n",
        " \n",
        "def plot_spectrogram(spectrogram, ax):\n",
        "  # Convert to frequencies to log scale and transpose so that the time is\n",
        "  # represented in the x-axis (columns).\n",
        "  log_spec = np.log(spectrogram.T)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)\n",
        " \n",
        " \n",
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.show()\n",
        " \n",
        " \n",
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  spectrogram = tf.expand_dims(spectrogram, -1)\n",
        "  label_id = tf.argmax(label == commands)\n",
        "  return spectrogram, label_id\n",
        " \n",
        " \n",
        "spectrogram_ds = waveform_ds.map(\n",
        "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)\n",
        " \n",
        " \n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
        "  ax.set_title(commands[label_id.numpy()])\n",
        "  ax.axis('off')\n",
        "  \n",
        "plt.show()\n",
        " \n",
        " \n",
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds\n",
        " \n",
        " \n",
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)\n",
        "print(\"test_ds\")\n",
        "print(type(train_ds)) \n",
        " \n",
        "batch_size = 64\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "test_ds = test_ds.batch(batch_size) \n",
        " \n",
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G-TeIYUJdJv",
        "outputId": "dbb879d3-5d38-43bd-92f8-b54c1c6450c4"
      },
      "outputs": [],
      "source": [
        "iterator = train_ds.__iter__()\n",
        "next_element = iterator.get_next()\n",
        "pt = next_element[0]\n",
        "en = next_element[1]\n",
        "print(pt.numpy().shape)\n",
        "print(en.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E9Y8ucDJevt",
        "outputId": "bc2bca74-a032-44eb-9ccc-c901b650a0b4"
      },
      "outputs": [],
      "source": [
        "iterator1 = val_ds.__iter__()\n",
        "next_element1 = iterator1.get_next()\n",
        "pt1 = next_element1[0]\n",
        "en1 = next_element1[1]\n",
        "print(pt1.numpy().shape)\n",
        "print(en1.numpy().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FseP7xZJgdL",
        "outputId": "c3c492c5-433c-4505-9f45-b5fa94689dfd"
      },
      "outputs": [],
      "source": [
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "norm_layer = preprocessing.Normalization()\n",
        "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1EViQPkJiCO"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pxkqTjpJj4T"
      },
      "outputs": [],
      "source": [
        "def CNN_model(f1, f2, f3, f4, k, a1, a2, d1, d2, op, ep, fitness):\n",
        "  model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    preprocessing.Resizing(32, 32), \n",
        "    norm_layer,\n",
        "  ])\n",
        "\n",
        "  model.add(Conv2D(input_shape=(32,32, 1),filters=f1,kernel_size=(k,k),padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f1,kernel_size=(k,k),padding=\"same\", activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d1))\n",
        "\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d2))\n",
        "\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d2))\n",
        "\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d1))\n",
        "\n",
        "\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d1))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(units=f4,activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(units=f4,activation=a1))\n",
        "  model.add(Dense(units=num_labels, activation=\"softmax\"))\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=op,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        "  )\n",
        "\n",
        "  EPOCHS = ep\n",
        "  \n",
        "  history = model.fit(\n",
        "    train_ds, \n",
        "    validation_data=val_ds,  \n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=10),\n",
        "  )\n",
        "  fitness.append((history.history[\"val_accuracy\"][-1], history.history[\"accuracy\"][-1]))\n",
        "  return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPtvheG1Jmfz"
      },
      "outputs": [],
      "source": [
        "# Generating the bounds list for every hyperparameter\n",
        "bounds = [\n",
        "    [16, 32, 64],                               # f1\n",
        "    [32, 64, 128],                              # f2\n",
        "    [32, 64, 128],                              # f3\n",
        "    [128, 256, 512],                            # f4\n",
        "    [3, 5],                                      # k\n",
        "    [\"relu\", \"selu\", \"elu\"],                    # a1\n",
        "    [\"relu\", \"selu\", \"elu\"],                    # a2\n",
        "    (0.1, 0.5),                                 # d1\n",
        "    (0.1, 0.5),                                 # d2\n",
        "    [\"adamax\", \"adadelta\", \"adam\", \"adagrad\"],  # op\n",
        "    [50, 60, 70, 80, 90, 100]                   # ep\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xhY8ADOJqKN",
        "outputId": "5d1473ff-ae15-45ad-96c5-204be71485b7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "pop_size = 15\n",
        "# Initializing a population of size 15\n",
        "population = [[random.choice(item) if type(item) is list else round(random.uniform(item[0], item[1]), 1) if type(item) is tuple else item for item in bounds] for _ in range(pop_size)]\n",
        "print(\"Population:\")\n",
        "for i, hyperparameters in enumerate(population):\n",
        "    print(\"Hyperparameters set\", i+1, \":\", hyperparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig3_3HeDWX9G"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbdQyQ4HhzDn",
        "outputId": "6dcd7c06-d860-4ef1-d5ea-ed919d69dd20"
      },
      "outputs": [],
      "source": [
        "population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJM7rSl3J_pc"
      },
      "outputs": [],
      "source": [
        "def mutation(individual, population, bounds, mutation_factor=0.8):\n",
        "\n",
        "    population_copy = population.copy()\n",
        "    population_copy.remove(individual)\n",
        "    a, b, c = random.sample(population_copy, 3)\n",
        "\n",
        "        \n",
        "    # Compute the difference between b and c\n",
        "    diff = [round(b_i - c_i, 1) if isinstance(b_i, (int, float)) else b_i for b_i, c_i in zip(b, c)]\n",
        "    \n",
        "    # Mutate the individual x by adding the difference multiplied by the mutation factor\n",
        "    mut_individual = [int(a_i + mutation_factor * d) if i in [0,1,2,3,4,10] and \n",
        "                        isinstance(a_i, (int, float)) else random.choice(bounds[i])  \n",
        "                        if i in [0,1,2,3,4,10] and not isinstance(a_i, (int, float)) else round(a_i + mutation_factor * d, 1)\n",
        "                        if isinstance(a_i, (int, float)) else random.choice(bounds[i]) for i,(a_i, d) in enumerate(zip(a, diff))]\n",
        "    \n",
        "    # make sure that f1, f2, f3, f4 are within (32, 256) bounds\n",
        "    for j in range(4):\n",
        "        if mut_individual[j] < 32:\n",
        "            mut_individual[j] = 32\n",
        "        elif mut_individual[j] > 256:\n",
        "            mut_individual[j] = 256\n",
        "    # Make sure dropout rate stays between (0.1, 0.5)\n",
        "    for j in [7,8]:\n",
        "        if mut_individual[j] <= 0:\n",
        "            mut_individual[j] = 0.1\n",
        "        elif mut_individual[j] >= 0.5:\n",
        "            mut_individual[j] = 0.5\n",
        "            \n",
        "    if mut_individual[4] < 3:\n",
        "        mut_individual[4] = 3\n",
        "            \n",
        "            \n",
        "\n",
        "    # Min 50 epochs\n",
        "    if mut_individual[10] < 50:\n",
        "        mut_individual[10] = 50\n",
        "    return mut_individual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OylLdZatJ5Mn"
      },
      "outputs": [],
      "source": [
        "def recombination(individual, population, bounds, CR=0.9):\n",
        "    # Mutate the individual first\n",
        "    new_individual = mutation(individual, population, bounds)\n",
        "    \n",
        "    # Pick a random index R in range 1 to n where n is the dimensionality of the problem being optimized.\n",
        "    R = random.randint(1, len(bounds))\n",
        "    \n",
        "    # Compute the agent's potentially new position\n",
        "    y = []\n",
        "    for i in range(len(bounds)):\n",
        "        # Pick a uniformly distributed random number r_i in range(0,1)\n",
        "        r = random.uniform(0, 1)\n",
        "        if (r < CR) or (i == R):\n",
        "            y_i = new_individual[i]\n",
        "        else:\n",
        "            y_i = individual[i]\n",
        "        y.append(y_i)\n",
        "    \n",
        "    # If f(y)>=f(x) then replace the agent x in the population with the improved or equal candidate solution y\n",
        "\n",
        "    fitness_y = []\n",
        "    fitness_x = []\n",
        "    print(y)\n",
        "    print(individual)\n",
        "    CNN_model(*y, fitness_y)\n",
        "    CNN_model(*individual, fitness_x)\n",
        "\n",
        "    # Comparing based on validation accuracy\n",
        "    if fitness_y[0][0] >= fitness_x[0][0]:\n",
        "        return y\n",
        "    else:\n",
        "        return individual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1QMmNFVlUW5",
        "outputId": "9bea0bad-3d62-485c-f356-373946af240f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJfyGsBuyAng",
        "outputId": "7985e388-f60c-418a-ee4b-f9e03fe95347"
      },
      "outputs": [],
      "source": [
        "iterations = 10\n",
        "\n",
        "with open(\"generation_info.txt\", \"w\") as f:\n",
        "    for iterator in range(iterations):  \n",
        "        for i in range(len(population)):\n",
        "            # The below call to recombination also has the mutation call within it so it mutates\n",
        "            new_individual = recombination(population[i], population, bounds)\n",
        "            population[i] = new_individual\n",
        "        print(\"Iteration\", iterator + 1, \"over\")\n",
        "        print(\"Current population: \", population)\n",
        "\n",
        "\n",
        "        fitness_gen = []\n",
        "        for item in population: # Do this for the current population of a generation\n",
        "            CNN_model(*item, fitness_gen)\n",
        "            max_valaccuracy_index = fitness_gen.index(max(fitness_gen))\n",
        "        f.write(\"Generation: \" + str(iterator + 1) + \"\\n\")\n",
        "        f.write(str(fitness_gen[max_valaccuracy_index]) + \"\\n\")\n",
        "        f.write(str(population[max_valaccuracy_index]) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgJF8lwFSVrm"
      },
      "outputs": [],
      "source": [
        "best = [36, 183, 256, 56, 6, 'elu', 'relu', 0.1, 0.2, 'adagrad', 114]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RAwAqLCSW6U",
        "outputId": "262ccace-3cb4-4c4a-b223-a2be2fb2e001"
      },
      "outputs": [],
      "source": [
        "fitnessbest = []\n",
        "best_model, history = CNN_model(*best, fitnessbest)\n",
        "best_model.evaluate(test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
