{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwmjLot9BfYU",
        "outputId": "0fdc84c1-e5d7-4ad8-bda3-999c1893df70"
      },
      "outputs": [],
      "source": [
        "!pip install python_speech_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PHdAgO5CqSKr",
        "outputId": "0b476678-e2ba-48ee-b1ca-6c438cd94199"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,LSTM,BatchNormalization,GlobalAveragePooling2D\n",
        "from keras.layers import Dropout,Dense,TimeDistributed\n",
        "from keras.models import Sequential\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "from python_speech_features import mfcc\n",
        "import pickle\n",
        "from keras.callbacks import ModelCheckpoint\n",
        " \n",
        "import librosa as lr\n",
        " \n",
        "data_dir = pathlib.Path('data/mini_speech_commands')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')\n",
        " \n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)\n",
        " \n",
        " \n",
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)\n",
        "print('Number of examples per label:',\n",
        "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
        "print('Example file tensor:', filenames[0])\n",
        " \n",
        " \n",
        " \n",
        "train_files = filenames[:6400]\n",
        "val_files = filenames[6400: 6400 + 1000]\n",
        "test_files = filenames[-600:]\n",
        " \n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))\n",
        " \n",
        " \n",
        "def decode_audio(audio_binary):\n",
        "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
        "  return tf.squeeze(audio, axis=-1)\n",
        " \n",
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        " \n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2] \n",
        " \n",
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  print(\"label\")\n",
        "  print(label)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  print(\"waveform\")\n",
        "  print(waveform)\n",
        "  return waveform, label\n",
        " \n",
        " \n",
        " \n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        " \n",
        " \n",
        " \n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
        "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  ax.plot(audio.numpy())\n",
        "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  ax.set_title(label)\n",
        " \n",
        "plt.show()\n",
        " \n",
        " \n",
        " \n",
        "def get_spectrogram(waveform):\n",
        "  # Padding for files with less than 16000 samples\n",
        "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        " \n",
        "  # Concatenate audio with padding so that all audio clips will be of the \n",
        "  # same length\n",
        "  waveform = tf.cast(waveform, tf.float32)\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "      \n",
        "  spectrogram = tf.abs(spectrogram)\n",
        " \n",
        "  return spectrogram\n",
        " \n",
        " \n",
        "for waveform, label in waveform_ds.take(1):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        " \n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Audio playback')\n",
        "display.display(display.Audio(waveform, rate=16000))\n",
        " \n",
        " \n",
        "def plot_spectrogram(spectrogram, ax):\n",
        "  # Convert to frequencies to log scale and transpose so that the time is\n",
        "  # represented in the x-axis (columns).\n",
        "  log_spec = np.log(spectrogram.T)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)\n",
        " \n",
        " \n",
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.show()\n",
        " \n",
        " \n",
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  spectrogram = tf.expand_dims(spectrogram, -1)\n",
        "  label_id = tf.argmax(label == commands)\n",
        "  return spectrogram, label_id\n",
        " \n",
        " \n",
        "spectrogram_ds = waveform_ds.map(\n",
        "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)\n",
        " \n",
        " \n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
        "  ax.set_title(commands[label_id.numpy()])\n",
        "  ax.axis('off')\n",
        "  \n",
        "plt.show()\n",
        " \n",
        " \n",
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds\n",
        " \n",
        " \n",
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)\n",
        "print(\"test_ds\")\n",
        "print(type(train_ds)) \n",
        " \n",
        "batch_size = 64\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "test_ds = test_ds.batch(batch_size) \n",
        " \n",
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpX6p3OrpzUx",
        "outputId": "71bf1315-5da2-4dd0-aee8-868333351b1b"
      },
      "outputs": [],
      "source": [
        "iterator = train_ds.__iter__()\n",
        "next_element = iterator.get_next()\n",
        "pt = next_element[0]\n",
        "en = next_element[1]\n",
        "print(pt.numpy().shape)\n",
        "print(en.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pWELHqysCe_",
        "outputId": "889a61b3-a51a-43c4-a024-3662e1c9752b"
      },
      "outputs": [],
      "source": [
        "iterator1 = val_ds.__iter__()\n",
        "next_element1 = iterator1.get_next()\n",
        "pt1 = next_element1[0]\n",
        "en1 = next_element1[1]\n",
        "print(pt1.numpy().shape)\n",
        "print(en1.numpy().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GlOJ7jRrGFw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4mFyDRP5dZL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from scipy.io import wavfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,LSTM,BatchNormalization,GlobalAveragePooling2D\n",
        "from keras.layers import Dropout,Dense,TimeDistributed\n",
        "from keras.models import Sequential\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "from python_speech_features import mfcc\n",
        "import pickle\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import librosa as lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeSanShI671x"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIqV5naeKbql",
        "outputId": "3f96ef45-68f7-4c10-cf11-dc8be512b539"
      },
      "outputs": [],
      "source": [
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "norm_layer = preprocessing.Normalization()\n",
        "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueKjpXWOjJ5o"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEk9qXHW4eXx"
      },
      "outputs": [],
      "source": [
        "def CNN_model( f1, f2, f3, f4, k, a1, a2, d1, d2, op, ep):\n",
        "  model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    preprocessing.Resizing(32, 32), \n",
        "    norm_layer,\n",
        "  ])\n",
        "\n",
        "  model.add(Conv2D(input_shape=(32,32, 1),filters=f1,kernel_size=(k,k),padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f1,kernel_size=(k,k),padding=\"same\", activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d1))\n",
        "\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d2))\n",
        "\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a2))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d2))\n",
        "\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f2, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d1))\n",
        "\n",
        "\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(Conv2D(filters=f3, kernel_size=(k,k), padding=\"same\", activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\n",
        "  model.add(Dropout(d1))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(units=f4,activation=a1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(units=f4,activation=a1))\n",
        "  model.add(Dense(units=num_labels, activation=\"softmax\"))\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=op,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        "  )\n",
        "  EPOCHS = ep\n",
        "  history = model.fit(\n",
        "    train_ds, \n",
        "    validation_data=val_ds,  \n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=6),\n",
        "  )\n",
        "  #store history values in global dic.\n",
        "  return model, history\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZRN4Zkh1Yu_"
      },
      "outputs": [],
      "source": [
        "from random import choice\n",
        "from random import uniform\n",
        "from numpy.random import randint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6oy-UhmslXQ"
      },
      "outputs": [],
      "source": [
        "def initialization():  \n",
        "  parameters = {}\n",
        "  f1 = choice([16, 32, 64])\n",
        "  parameters[\"f1\"] = f1\n",
        "  f2 = choice([32, 64, 128])\n",
        "  parameters[\"f2\"] = f2\n",
        "  f3 = choice([32, 64, 128])\n",
        "  parameters[\"f3\"] = f3\n",
        "  f4 = choice([128, 256, 512])\n",
        "  parameters[\"f4\"] = f4\n",
        "  k = choice([3,5])\n",
        "  parameters[\"k\"] = k\n",
        "  a1 = choice([\"relu\", \"selu\", \"elu\"])\n",
        "  parameters[\"a1\"] = a1\n",
        "  a2 = choice([\"relu\", \"selu\", \"elu\"])\n",
        "  parameters[\"a2\"] = a2\n",
        "  d1 = round(uniform(0.1, 0.5), 1)\n",
        "  parameters[\"d1\"] = d1\n",
        "  d2 = round(uniform(0.1, 0.5), 1)\n",
        "  parameters[\"d2\"] = d2\n",
        "  op = choice([\"adamax\", \"adadelta\", \"adam\", \"adagrad\"])\n",
        "  parameters[\"op\"] = op\n",
        "  ep = randint(50,100)\n",
        "  parameters[\"ep\"] = ep\n",
        "  return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87ThhBE96zCm"
      },
      "outputs": [],
      "source": [
        "def generate_population(n):\n",
        "  population = []\n",
        "  for i in range(n):\n",
        "    chromosome = initialization()\n",
        "    population.append(chromosome)\n",
        "  return population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SomjG3tQ310Z"
      },
      "outputs": [],
      "source": [
        "# Fitness evaluation metric: Classification Accuracy \n",
        "def fitness_evaluation(model):\n",
        "  metrics = model.evaluate(test_ds)\n",
        "  print(f\"metrics:{metrics}\")\n",
        "  return metrics[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d8QyqTQ35O1"
      },
      "outputs": [],
      "source": [
        "# Roulette wheel selection method\n",
        "def selection(population_fitness):\n",
        "  total = sum(population_fitness)a\n",
        "  percentage = [round((x/total) * 100) for x in population_fitness]\n",
        "  selection_wheel = []\n",
        "  for pop_index,num in enumerate(percentage):\n",
        "    selection_wheel.extend([pop_index]*num)\n",
        "  parent1_ind = choice(selection_wheel)\n",
        "  parent2_ind = choice(selection_wheel)\n",
        "  return [parent1_ind, parent2_ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tw9NKnz379Q"
      },
      "outputs": [],
      "source": [
        "def crossover(parent1, parent2):\n",
        "  child1 = {}\n",
        "  child2 = {}\n",
        "\n",
        "  child1[\"f1\"] = choice([parent1[\"f1\"], parent2[\"f1\"]])\n",
        "  child1[\"f2\"] = choice([parent1[\"f2\"], parent2[\"f2\"]])\n",
        "  child1[\"f3\"] = choice([parent1[\"f3\"], parent2[\"f3\"]])\n",
        "  child1[\"f4\"] = choice([parent1[\"f4\"], parent2[\"f4\"]])\n",
        "\n",
        "  child2[\"f1\"] = choice([parent1[\"f1\"], parent2[\"f1\"]])\n",
        "  child2[\"f2\"] = choice([parent1[\"f2\"], parent2[\"f2\"]])\n",
        "  child2[\"f3\"] = choice([parent1[\"f3\"], parent2[\"f3\"]])\n",
        "  child2[\"f4\"] = choice([parent1[\"f4\"], parent2[\"f4\"]])\n",
        "\n",
        "  child1[\"k\"] = choice([parent1[\"k\"], parent2[\"k\"]])\n",
        "  child2[\"k\"] = choice([parent1[\"k\"], parent2[\"k\"]])\n",
        "\n",
        "  child1[\"a1\"] = parent1[\"a2\"]\n",
        "  child2[\"a1\"] = parent2[\"a2\"]\n",
        "\n",
        "  child1[\"a2\"] = parent2[\"a1\"]\n",
        "  child2[\"a2\"] = parent1[\"a1\"]\n",
        "\n",
        "  child1[\"d1\"] = parent1[\"d1\"]\n",
        "  child2[\"d1\"] = parent2[\"d1\"]\n",
        "\n",
        "  child1[\"d2\"] = parent2[\"d2\"]\n",
        "  child2[\"d2\"] = parent1[\"d2\"]\n",
        "\n",
        "  child1[\"op\"] = parent2[\"op\"]\n",
        "  child2[\"op\"] = parent1[\"op\"]\n",
        "\n",
        "  child1[\"ep\"] = parent1[\"ep\"]\n",
        "  child2[\"ep\"] = parent2[\"ep\"]\n",
        "  return [child1, child2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvHJT5d23-AN"
      },
      "outputs": [],
      "source": [
        "def mutation(chromosome):\n",
        "  flag = randint(0,40)\n",
        "  if flag <= 20:\n",
        "    chromosome[\"ep\"] += randint(0, 10)\n",
        "  return chromosome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHz68ueB4Bv6",
        "outputId": "b6dfbd9e-081f-4421-c6b1-617411346c41"
      },
      "outputs": [],
      "source": [
        "generations = 12\n",
        "threshold = 90\n",
        "num_pop = 10\n",
        "\n",
        "population = generate_population(num_pop)\n",
        "acc_best = []\n",
        "par_total = []\n",
        "acc_total = []\n",
        "par_best = []\n",
        "for generation in range(generations):\n",
        "\n",
        "  population_fitness = []\n",
        "  per1=[]\n",
        "  for chromosome in population:\n",
        "    f1 = chromosome[\"f1\"]\n",
        "    f2 = chromosome[\"f2\"]\n",
        "    f3 = chromosome[\"f3\"]\n",
        "    f4 = chromosome[\"f4\"]\n",
        "    k = chromosome[\"k\"]\n",
        "    a1 = chromosome[\"a1\"]\n",
        "    a2 = chromosome[\"a2\"]\n",
        "    d1 = chromosome[\"d1\"]\n",
        "    d2 = chromosome[\"d2\"]\n",
        "    op = chromosome[\"op\"]\n",
        "    ep = chromosome[\"ep\"]\n",
        "\n",
        "    try:\n",
        "      model = CNN_model(f1, f2, f3, f4, k, a1, a2, d1, d2, op, ep)\n",
        "      acc = fitness_evaluation(model)\n",
        "      par_total.append(chromosome)\n",
        "      acc_total.append(acc)\n",
        "      per1.append(chromosome)\n",
        "      print(\"Parameters: \", chromosome)\n",
        "      print(\"Accuracy: \", round(acc,3))\n",
        "    except:\n",
        "      acc=0\n",
        "      print(\"Parameters: \", chromosome)\n",
        "      print(\"Invalid parameters - Build fail\")\n",
        "\n",
        "    population_fitness.append(acc)\n",
        "  print(population_fitness)\n",
        "  parents_ind = selection(population_fitness)\n",
        "  parent1 = population[parents_ind[0]]\n",
        "  parent2 = population[parents_ind[1]]\n",
        "\n",
        "  children = crossover(parent1, parent2)\n",
        "  child1 = mutation(children[0])\n",
        "  child2 = mutation(children[1])\n",
        "\n",
        "  population.append(child1)\n",
        "  population.append(child2)\n",
        "\n",
        "  print(\"Generation \", generation+1,\" Outcome: \")\n",
        "  if max(population_fitness) >= threshold:\n",
        "    print(\"Obtained desired accuracy: \", max(population_fitness))\n",
        "    break\n",
        "  else:\n",
        "    print(\"Maximum accuracy in generation {} : {}\".format(generation+1, max(population_fitness)))\n",
        "    max_pop = max(population_fitness)\n",
        "    c1=population_fitness.index(max_pop)\n",
        "    par_best.append(per1[c1])\n",
        "    acc_best.append(max_pop)\n",
        "\n",
        "  first_min = min(population_fitness)\n",
        "  first_min_ind = population_fitness.index(first_min)\n",
        "  population.remove(population[first_min_ind])\n",
        "  second_min = min(population_fitness)\n",
        "  second_min_ind = population_fitness.index(second_min)\n",
        "  population.remove(population[second_min_ind])\n",
        "\n",
        "print(par_total)\n",
        "print(acc_total)\n",
        "print(par_best)\n",
        "print(acc_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pACtZPGxVhc"
      },
      "outputs": [],
      "source": [
        "var1=len(acc_best)\n",
        "acc_best.sort()\n",
        "print(acc_best[var1-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12wploAK1JAa",
        "outputId": "1e4f4f4b-b9ac-474d-f204-3dc3cd2291d4"
      },
      "outputs": [],
      "source": [
        "best_model, best_history = CNN_model(*[32, 128, 64, 128, 3, 'selu', 'relu', 0.4, 0.2, 'adam', 94])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riQe2Ou93fCe",
        "outputId": "d0adce7b-fb5e-488d-e390-140648ad3391"
      },
      "outputs": [],
      "source": [
        "best_model.evaluate(test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "0bd6827e5b9b024a8afcecbb4b32b3f39bbe94aa5ba060866c09f0f3ec848126"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
